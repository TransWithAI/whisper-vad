# Configuration for Encoder-Only VAD with Lightweight Decoder (Refinement I - Advanced)

# Model configuration
model:
  whisper_model_name: "openai/whisper-small"
  freeze_encoder: true  # Freeze encoder to maintain learned audio processing
  decoder_layers: 2      # Number of decoder layers (keep lightweight)
  decoder_heads: 12      # Number of attention heads (scaled for whisper-small)
  decoder_ff_dim: 3072   # Feed-forward dimension (scaled for whisper-small)
  dropout: 0.1

# Data configuration
data:
  dataset_path: "/path/to/your/vad_dataset"
  num_workers: 16
  max_train_samples: null  # Set a number for debugging
  max_val_samples: null    # Set a number for debugging
  # Streaming configuration (NEW)
  streaming: false  # Set to false for proper epoch progress tracking
  # If streaming is true, provide dataset lengths for progress bars:
  train_dataset_length: 54469  # Your training dataset has 54469 samples
  val_dataset_length: 6053      # Your validation dataset has 6053 samples

# Training configuration
training:
  batch_size: 128
  learning_rate: 0.0015
  weight_decay: 0.00001
  warmup_epochs: 0.2  # Warmup for first 2 epochs
  max_epochs: 20
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  val_check_interval: 1.0
  log_every_n_steps: 10

# Loss configuration
loss:
  focal_alpha: 0.25  # Weight for positive class
  focal_gamma: 2.0   # Focusing parameter

# Monitoring
monitor_metric: "val/f1"
monitor_mode: "max"
save_top_k: 3

# Early stopping
early_stopping:
  enabled: true
  patience: 15  # More patience for complex model

# Hardware
accelerator: "auto"  # auto, gpu, cpu
devices: 1
precision: bf16-true  # 16 for mixed precision, 32 for full precision

# Logging
logger: "tensorboard"  # tensorboard or wandb
project_name: "whisper-vad"
experiment_name: "encoder_decoder_small"

# Random seed
seed: 42